# Law Document Crawler

A tool for automated crawling and downloading of legal documents from luatvietnam.vn

## ğŸ› ï¸ Main Features

- âœ… Automated Google login with session management
- ğŸŒ Single URL and batch processing support
- ğŸ–¥ï¸ Interactive menu-driven interface
- âš™ï¸ Configurable batch settings
- ğŸ”„ Progress tracking with resume capability
- ğŸš€ Automatic system resource optimization
- ğŸ—‘ï¸ Duplicate file detection and removal
- ğŸ“‚ Dynamic filename generation with collision avoidance

---

## ğŸ“‹ Menu Structure

### 1ï¸âƒ£ Login Options

- ğŸ” Use saved credentials
- ğŸ› ï¸ Setup/update credentials
- ğŸª Automatic cookie management
- ğŸ”„ Session persistence

### 2ï¸âƒ£ Process Single URL

- ğŸ” Direct document download
- ğŸ§  Automatic format detection
- ğŸ“ Progress tracking
- ğŸ”„ Filename sanitization to avoid invalid characters

### 3ï¸âƒ£ Batch Processing

```bash
- ğŸ“‘ Process all Excel files in 'batches' folder
- ğŸ“‚ Select specific Excel file
- âš™ï¸ Configure batch settings
- ğŸ“Š Show download progress
- ğŸ” Retry failed downloads with retry_mode
```

### 4ï¸âƒ£ Batch Configuration

```bash
- ğŸ” Show current settings
- âš™ï¸ Configure settings interactively
- ğŸ¤– Auto-configure based on system
- ğŸ”„ Reset to defaults
```

### 5ï¸âƒ£ Cleanup Operations

```bash
- ğŸ—‘ï¸ Remove duplicate PDFs
- ğŸ” Remove lock files
- ğŸ”„ Automatic cleanup on exit
```

---

## âš™ï¸ **Technical Details**

### ğŸ“‚ Folder Structure

- **`crawl/`**: Main crawling logic, including file downloads and batch processing.
- **`downloads/`**: Contains all downloaded documents.
- **`logs/`**: Logs application events and errors.
- **`utils/`**: Utility functions for logging, file formatting, and session management.
- **`main.py`**: Entry point for application execution.
- **`config.json`**: Stores configurations like credentials, batch settings, and download preferences.

### ğŸ”‘ File Naming Mechanism

- File names are formatted by removing invalid characters and limiting the length to 255 characters.
- Each URL is assigned a **hash (URL hash)** to ensure uniqueness.
- Filename structure: `prefix_timestamp_urlhash.extension`

**Example:** `quyet dinh 1756 qd bgtvt bo giao thong van tai 53709 d1.doc`

### ğŸ›‘ Duplicate Detection

- Duplicate file removal occurs after all files have been downloaded.
- The `remove_duplicate_documents()` function scans for duplicate PDFs when DOC or DOCX versions exist.
- The detection is based on file content, not just the filename.

### âš™ï¸ Batch Configuration Settings

Configurable parameters:
- **`chunk_size`**: The size of each data chunk.
- **`max_workers`**: The number of concurrent threads.
- **`batch_size`**: The number of URLs processed simultaneously.
- **`max_tabs`**: The maximum number of browser tabs.
- **`retry_mode`**: Enable or disable retry for failed downloads.

### ğŸ§  Retry Mode (`retry_mode`)

- **Purpose:** Retry downloads when encountering errors (e.g., network issues, server failures).
- **Values:**
  - `True`: Retry downloads up to the configured limit.
  - `False`: Skip failed files.

### ğŸ–¼ï¸ File Naming and Deduplication

- File names are generated based on the URL's hash.
- Duplicate files are removed post-download using `remove_duplicate_documents()`.

---

## âš™ï¸ **Setup Guide**

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the crawler

```bash
python main.py [--debug] [--no-headless]
```

### 3ï¸âƒ£ Configure Google account through the interactive menu

### 4ï¸âƒ£ Place files in `batches/` folder

### 5ï¸âƒ£ Start crawling with batch processing

---

## ğŸ–¥ï¸ **Command Line Options**

```bash
python main.py [options]

Options:
  --debug         Enable debug mode
  --no-headless   Disable headless browser mode
```

---

## ğŸ› ï¸ **System Requirements**

- ğŸ Python 3.7+
- ğŸŒ Google Chrome
- ğŸ“¡ Stable internet connection
- ğŸ“‘ Excel files with URLs


